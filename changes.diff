diff --color -ruN '--exclude=a.py' '--exclude=a.ipynb' '--exclude=b.ipynb' '--exclude=reacher.cfg' '--exclude=config.cfg' archive/agents/fgsfdqn.py current/agents/fgsfdqn.py
--- archive/agents/fgsfdqn.py	2026-01-04 18:41:31.273112901 +0530
+++ current/agents/fgsfdqn.py	2026-01-04 18:41:14.382079435 +0530
@@ -48,24 +48,41 @@
             self.sf.update_single_sample(sub_batch, task_i, task_c)
 
     def train_agent(self, s, s_enc, a, r, s1, s1_enc, gamma):
-        # Algorithm 1: Sequential Update
-        if self.algorithm != 'alg1':
-            # Store data for Alg 2/3
-            phi = self.phi(s, a, s1)
-            if isinstance(phi, torch.Tensor): phi = phi.detach().cpu().numpy()
-            self.buffer.append(s_enc, a, phi, s1_enc, gamma)
-            self.sf.update_reward(phi, r, self.task_index)
-            return
-
+        # Store transition + reward update for all algorithms
         phi = self.phi(s, a, s1)
-        if isinstance(phi, torch.Tensor): phi = phi.detach().cpu().numpy()
+        if isinstance(phi, torch.Tensor):
+            phi = phi.detach().cpu().numpy()
         self.sf.update_reward(phi, r, self.task_index)
         self.buffer.append(s_enc, a, phi, s1_enc, gamma)
-        
-        batch = self.buffer.replay()
-        if batch:
-            # Group by optimal Prior c
-            self._update_batch_grouped_by_prior(batch, self.task_index)
+
+        if self.algorithm == 'alg1':
+            # Algorithm 1: Sequential, single-sample update
+            batch = self.buffer.replay()
+            if batch:
+                self._update_batch_grouped_by_prior(batch, self.task_index)
+            return
+
+        if self.algorithm == 'alg4':
+            # Algorithm 4: Algorithm 1 + averaging (as in Algorithm 3)
+            # Uses a pivot (s,a) and a conditional batch to compute an averaged target
+            # and an averaged prior-policy index.
+            pivot = self.buffer.sample_pivot()
+            if not pivot:
+                return
+
+            p_s, p_a, _, _, _ = pivot
+            cond_batch = self.buffer.sample_conditional(p_s, p_a, self.n_averaging)
+            if not cond_batch:
+                return
+
+            _, _, _, c_next_states, _ = cond_batch
+            c = self.sf.get_averaged_gpi_policy_index(c_next_states, self.task_index)
+            task_c = c if c != self.task_index else None
+            self.sf.update_averaged(p_s, p_a, cond_batch, self.task_index, task_c)
+            return
+
+        # For Alg 2/3 (randomized training), updates happen inside train_randomized
+        return
 
     def train_randomized(self, train_tasks, n_total_steps, viewers=None, n_view_ev=None):
         if viewers is None: viewers = [None] * len(train_tasks)
@@ -108,9 +125,7 @@
                     if cond_batch:
                         _, _, _, c_next_states, _ = cond_batch
                         
-                        # Use Mean Feature of (encoded) next states to select c
-                        mean_next_state_feature = np.mean(c_next_states, axis=0)
-                        c = self._get_gpi_policy(mean_next_state_feature, i)[0]
+                        c = self.sf.get_averaged_gpi_policy_index(c_next_states, i)
                         
                         task_c = c if c != i else None
                         self.sf.update_averaged(p_s, p_a, cond_batch, i, task_c)
\ No newline at end of file
diff --color -ruN '--exclude=a.py' '--exclude=a.ipynb' '--exclude=b.ipynb' '--exclude=reacher.cfg' '--exclude=config.cfg' archive/features/deep_fg.py current/features/deep_fg.py
--- archive/features/deep_fg.py	2026-01-03 22:10:16.000000000 +0530
+++ current/features/deep_fg.py	2026-01-04 18:50:09.982351851 +0530
@@ -53,6 +53,42 @@
         # Argmax
         next_actions = torch.argmax(q_values, dim=1)
         return next_actions
+    
+    def get_averaged_gpi_policy_index(self, next_states, task_index):
+        """
+        Determines the prior policy 'c' by averaging the SFs of the batch 
+        of next states and then applying GPI.
+        """
+        if not isinstance(next_states, torch.Tensor):
+            next_states = self._to_tensor(next_states)
+
+        # Get the reward weights for the current task
+        w = torch.from_numpy(self.fit_w[task_index]).float().to(self.device)
+        
+        best_policy_idx = -1
+        best_q_val = -float('inf')
+
+        # Evaluate every policy k
+        for k in range(self.n_tasks):
+            model, _, _ = self.psi[k]
+            model.eval()
+            with torch.no_grad():
+                # Compute SFs for all samples in batch [N, Actions, Features]
+                all_sfs = model(next_states)
+                
+                # Average the SFs across the batch dimension
+                # This approximates E[xi(s', a')]
+                avg_sf = torch.mean(all_sfs, dim=0) 
+                
+                # Compute Q-values using current task weights
+                q_vals = torch.matmul(avg_sf, w) # [Actions]
+                max_q = torch.max(q_vals).item()
+                
+                if max_q > best_q_val:
+                    best_q_val = max_q
+                    best_policy_idx = k
+                    
+        return best_policy_idx
 
     def update_single_sample(self, transition, task_i, task_c):
         """
@@ -113,10 +149,6 @@
     def update_averaged(self, pivot_state, pivot_action, conditional_batch, task_i, task_c):
         """
         Implements averaging for bellman update over N samples.
-        
-        pivot_state: (1, dim)
-        pivot_action: int
-        conditional_batch: tuple of (N, dim) tensors containing N samples for (s, a)
         """
         _, _, c_phis, c_next_states, c_gammas = conditional_batch
         
@@ -139,25 +171,33 @@
             model.train()
             optimizer.zero_grad()
             
-            # Prediction: xi_k(s, a) (Single value for the pivot)
+            # xi_k(s, a) (Single value for the pivot)
             pred_pivot = model(pivot_state_t) # [1, n_actions, dim]
             xi_s = pred_pivot[0, pivot_action, :] # [dim]
             
-            # Compute Averaged Target over N samples
-            if k == task_i:
-                next_acts = self._get_next_actions_gpi(c_nexts, task_i)
-            else:
-                next_acts = self._get_next_action_prior(c_nexts, k)
+            # Averaged Target 
+            # a_hat = argmax_a' ( E[xi(s', a')] * w_i )
+            w_selection = torch.from_numpy(self.fit_w[task_i]).float().to(self.device)
+            
+            # Forward pass all next states [N, A, D]
+            pred_next_all = model(c_nexts) 
+            
+            # Compute Mean Feature Vector over batch [A, D]
+            xi_next_mean = torch.mean(pred_next_all, dim=0) 
+            
+            # Select action that maximizes value on the mean features
+            q_next_mean = torch.matmul(xi_next_mean, w_selection)
+            hat_a = torch.argmax(q_next_mean).item()
             
-            pred_next_all = model(c_nexts) # [N, n_actions, dim]
+            # Gather the specific SFs for that chosen action from the full batch
             indices = torch.arange(c_nexts.shape[0])
-            xi_next = pred_next_all[indices, next_acts, :] # [N, dim]
+            xi_next_selected = pred_next_all[indices, hat_a, :] # [N, dim]
             
-            # Element-wise target: phi_p + gamma_p * xi(s'_p)
-            targets_N = c_phis + c_gammas * xi_next
+            # Element-wise target: phi_p + gamma_p * xi(s'_p, hat{a})
+            targets_N = c_phis + c_gammas * xi_next_selected
             
-            # Average over N
-            target_bar = torch.mean(targets_N, dim=0) # [dim]
+            # Average the calculated targets
+            target_bar = torch.mean(targets_N, dim=0)
             
             diff = target_bar - xi_s
             loss = 0.5 * (diff.pow(2)).mean()
diff --color -ruN '--exclude=a.py' '--exclude=a.ipynb' '--exclude=b.ipynb' '--exclude=reacher.cfg' '--exclude=config.cfg' archive/features/deep.py current/features/deep.py
--- archive/features/deep.py	2026-01-03 22:10:16.000000000 +0530
+++ current/features/deep.py	2026-01-04 16:34:52.392298364 +0530
@@ -6,45 +6,38 @@
 import torch.optim as optim
 from features.successor import SF
 
-# class SFNetwork(nn.Module):
-#     def __init__(self, input_dim, n_actions, n_features):
-#         super(SFNetwork, self).__init__()
-#         self.input_dim = input_dim
-#         self.n_actions = n_actions
-#         self.n_features = n_features
-#         # Simple MLP
-#         self.net = nn.Sequential(
-#             nn.Linear(input_dim, 128),
-#             nn.SELU(),
-#             nn.Linear(128, 128),
-#             nn.SELU(),
-#             nn.Linear(128, n_actions * n_features)
-#         )
-#         # Initialize weights
-#         self._initialize_weights()
+class SFNetwork1(nn.Module):
+    def __init__(self, input_dim, n_actions, n_features):
+        super(SFNetwork1, self).__init__()
+        self.input_dim = input_dim
+        self.n_actions = n_actions
+        self.n_features = n_features
+        # Simple MLP
+        self.net = nn.Sequential(
+            nn.Linear(input_dim, 128),
+            nn.SELU(),
+            nn.Linear(128, 128),
+            nn.SELU(),
+            nn.Linear(128, n_actions * n_features)
+        )
+        # Initialize weights
+        self._initialize_weights()
         
-#     def _initialize_weights(self):
-#         for m in self.modules():
-#             if isinstance(m, nn.Linear):
-#                 nn.init.normal_(m.weight, 0, np.sqrt(1.0 / m.weight.shape[1]))
-#                 if m.bias is not None:
-#                     nn.init.constant_(m.bias, 0)
+    def _initialize_weights(self):
+        for m in self.modules():
+            if isinstance(m, nn.Linear):
+                nn.init.normal_(m.weight, 0, np.sqrt(1.0 / m.weight.shape[1]))
+                if m.bias is not None:
+                    nn.init.constant_(m.bias, 0)
     
-#     def forward(self, x):
-#         out = self.net(x)
-#         # Reshape to [batch, n_actions, n_features]
-#         return out.view(-1, self.n_actions, self.n_features)
+    def forward(self, x):
+        out = self.net(x)
+        # Reshape to [batch, n_actions, n_features]
+        return out.view(-1, self.n_actions, self.n_features)
 
-class SFNetwork(nn.Module):
+class SFNetwork2(nn.Module):
     def __init__(self, input_dim, n_actions, n_features, hidden_dims=[256, 256]):
-        """
-        Args:
-            state_dim (int): Dimension of the input state vector (e.g., 11 for Reacher-v4).
-            input_dim (int): Dimension of the successor features (phi).
-            n_actions (int): Number of discrete actions (default 9 based on your snippet).
-            hidden_dims (list): List of neurons for hidden layers.
-        """
-        super(SFNetwork, self).__init__()
+        super(SFNetwork2, self).__init__()
         self.n_actions = n_actions
         self.n_features = n_features
 
@@ -103,8 +96,18 @@
             self.input_dim = task.encode_dim()
 
         # Build SF network
-        model = SFNetwork(self.input_dim, self.n_actions, self.n_features).to(self.device)
-        target_model = SFNetwork(self.input_dim, self.n_actions, self.n_features).to(self.device)
+        model = None
+        target_model = None
+        if task._tasktype() == 0: # gridworld
+            model = SFNetwork1(self.input_dim, self.n_actions, self.n_features).to(self.device)
+            target_model = SFNetwork1(self.input_dim, self.n_actions, self.n_features).to(self.device)
+        elif task._tasktype() == 1: # reacher
+            model = SFNetwork2(self.input_dim, self.n_actions, self.n_features).to(self.device)
+            target_model = SFNetwork2(self.input_dim, self.n_actions, self.n_features).to(self.device)
+        else:
+            model = SFNetwork1(self.input_dim, self.n_actions, self.n_features).to(self.device)
+            target_model = SFNetwork1(self.input_dim, self.n_actions, self.n_features).to(self.device)
+            
         target_model.load_state_dict(model.state_dict())
         optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)
 
diff --color -ruN '--exclude=a.py' '--exclude=a.ipynb' '--exclude=b.ipynb' '--exclude=reacher.cfg' '--exclude=config.cfg' archive/tasks/gridworld.py current/tasks/gridworld.py
--- archive/tasks/gridworld.py	2026-01-03 22:10:16.000000000 +0530
+++ current/tasks/gridworld.py	2026-01-04 16:35:19.861224749 +0530
@@ -60,6 +60,9 @@
     def clone(self):
         return Shapes(self.maze, self.shape_rewards)
 
+    def _tasktype(self):
+        return 0
+    
     def initialize(self):
         self.state = (random.choice(self.initial), tuple(0 for _ in range(len(self.shape_ids))))
         return self.state
diff --color -ruN '--exclude=a.py' '--exclude=a.ipynb' '--exclude=b.ipynb' '--exclude=reacher.cfg' '--exclude=config.cfg' archive/tasks/reacher.py current/tasks/reacher.py
--- archive/tasks/reacher.py	2026-01-03 22:10:16.000000000 +0530
+++ current/tasks/reacher.py	2026-01-04 16:35:26.079208086 +0530
@@ -23,7 +23,10 @@
         for a1 in actions:
             for a2 in actions:
                 self.action_dict[len(self.action_dict)] = (a1, a2)
-        
+    
+    def _tasktype(self):
+        return 1
+    
     def clone(self):
         return Reacher(self.target_positions, self.task_index, self.include_target_in_state, self.device)
     
diff --color -ruN '--exclude=a.py' '--exclude=a.ipynb' '--exclude=b.ipynb' '--exclude=reacher.cfg' '--exclude=config.cfg' archive/tasks/task.py current/tasks/task.py
--- archive/tasks/task.py	2026-01-03 22:10:16.000000000 +0530
+++ current/tasks/task.py	2026-01-04 16:35:18.414228627 +0530
@@ -13,6 +13,9 @@
         """
         raise NotImplementedError
 
+    def _tasktype(self):
+        return NotImplementedError
+
     def initialize(self):
         """
         Resets the state of the environment.
