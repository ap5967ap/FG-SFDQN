{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a71a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import configparser\n",
    "from tasks.gridworld import Shapes\n",
    "from features.deep_fg import DeepFGSF\n",
    "from agents.fgsfdqn import FGSFDQN\n",
    "from agents.buffer import ConditionalReplayBuffer, ReplayBuffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4112612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path=\"configs/config.cfg\"):\n",
    "    cfg = configparser.ConfigParser()\n",
    "    cfg.read(path)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def setup_tasks(cfg):\n",
    "    \"\"\"\n",
    "    Creates training and testing tasks with diverse reward structures.\n",
    "    \"\"\"\n",
    "    maze = np.array(eval(cfg[\"TASK\"][\"maze\"]))\n",
    "    task_train_1 = Shapes(maze=maze, shape_rewards={'1': 10, '2': 1,  '3': 1})\n",
    "    task_train_2 = Shapes(maze=maze, shape_rewards={'1': 1,  '2': 10, '3': 1})\n",
    "    task_train_3 = Shapes(maze=maze, shape_rewards={'1': 1,  '2': 1,  '3': 10})\n",
    "    \n",
    "    task_train_4 = Shapes(maze=maze, shape_rewards={'1': 5, '2': 5, '3': 0})\n",
    "    task_train_5 = Shapes(maze=maze, shape_rewards={'1': 0, '2': 5, '3': 5})\n",
    "    task_train_6 = Shapes(maze=maze, shape_rewards={'1': 5, '2': 0, '3': 5})\n",
    "\n",
    "    train_tasks = [task_train_1, task_train_2, task_train_3,task_train_4, task_train_5, task_train_6]\n",
    "\n",
    "    test_task = Shapes(maze=maze, shape_rewards={'1': 10, '2': 10, '3': 10})\n",
    "    \n",
    "    return train_tasks, test_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dec0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = load_config()\n",
    "n_samples = int(cfg[\"GENERAL\"].get(\"n_samples\", 20000))\n",
    "buffer_size = int(cfg[\"GENERAL\"].get(\"buffer_size\", 200000))\n",
    "gamma       = float(cfg[\"AGENT\"][\"gamma\"])\n",
    "epsilon     = float(cfg[\"AGENT\"][\"epsilon\"])\n",
    "T           = int(cfg[\"AGENT\"][\"T\"])\n",
    "lr_sf       = float(cfg[\"SFQL\"][\"learning_rate\"])\n",
    "lr_w        = float(cfg[\"SFQL\"][\"learning_rate_w\"])\n",
    "algorithm   = cfg[\"FGSF\"].get(\"algorithm\", \"alg3\")\n",
    "n_averaging = int(cfg[\"FGSF\"].get(\"n_averaging\",5))\n",
    "n_batch     = int(cfg[\"GENERAL\"][\"n_batch\"])\n",
    "\n",
    "train_tasks, test_task = setup_tasks(cfg)\n",
    "input_dim = train_tasks[0].encode_dim()\n",
    "n_actions = train_tasks[0].action_count()\n",
    "n_features = train_tasks[0].feature_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720c429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = DeepFGSF(\n",
    "    input_dim=input_dim,\n",
    "    n_actions=n_actions,\n",
    "    n_features=n_features,\n",
    "    learning_rate=lr_sf,\n",
    "    learning_rate_w=lr_w,\n",
    "    target_update_ev=1000,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    use_true_reward=cfg[\"SFQL\"].getboolean(\"use_true_reward\", False)\n",
    ")\n",
    "buffer = None\n",
    "if algorithm == \"alg3\":\n",
    "    buffer = ConditionalReplayBuffer(\n",
    "        n_samples=buffer_size,\n",
    "        n_batch=n_batch\n",
    "    )\n",
    "else:\n",
    "    buffer = ReplayBuffer(n_samples=buffer_size, n_batch=n_batch)\n",
    "    \n",
    "\n",
    "agent = FGSFDQN(\n",
    "    deep_sf=sf,\n",
    "    buffer=buffer,\n",
    "    gamma=gamma,\n",
    "    T=T,\n",
    "    epsilon=epsilon,\n",
    "    epsilon_decay=0.99995, \n",
    "    epsilon_min=0.05,\n",
    "    encoding=\"task\",\n",
    "    algorithm=algorithm,      \n",
    "    n_averaging=n_averaging,  \n",
    "    print_ev=2000,\n",
    "    save_ev=200,\n",
    "    use_gpi=True\n",
    ")\n",
    "\n",
    "agent.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15eccd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_randomized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_tasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_total_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_tasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mviewers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/adv_rl (Copy)/agents/fgsfdqn.py:89\u001b[0m, in \u001b[0;36mFGSFDQN.train_randomized\u001b[0;34m(self, train_tasks, n_total_steps, viewers, n_view_ev)\u001b[0m\n\u001b[1;32m     87\u001b[0m i \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(train_tasks))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_active_training_task(i, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviewers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_view_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malg2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Algorithm 2: Randomized, Single Sample\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mreplay()\n",
      "File \u001b[0;32m~/Downloads/adv_rl (Copy)/agents/agent.py:196\u001b[0m, in \u001b[0;36mAgent.next_sample\u001b[0;34m(self, viewer, n_view_ev)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_reward_hist\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_reward)  \n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# compute the Q-values in the current state\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_Q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# choose an action using the epsilon-greedy policy\u001b[39;00m\n\u001b[1;32m    199\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epsilon_greedy(q)\n",
      "File \u001b[0;32m~/Downloads/adv_rl (Copy)/agents/sfdqn.py:40\u001b[0m, in \u001b[0;36mSFDQN.get_Q_values\u001b[0;34m(self, s, s_enc)\u001b[0m\n\u001b[1;32m     38\u001b[0m     s_enc_arr \u001b[38;5;241m=\u001b[39m s_enc_arr[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]  \u001b[38;5;66;03m# [1, ...]\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Call SF.GPI with numpy (we made SF.GPI expect numpy)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m q, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPI\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_enc_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_counters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_gpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpi:\n\u001b[1;32m     42\u001b[0m     c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_index)\n",
      "File \u001b[0;32m~/Downloads/adv_rl (Copy)/features/successor.py:259\u001b[0m, in \u001b[0;36mSF.GPI\u001b[0;34m(self, state, task_index, update_counters)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mGPI\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, task_index, update_counters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Implements generalized policy improvement according to [1]. \u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    np.ndarray : the tasks that are active in each state of state_batch in GPi\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     q, task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPI_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_w\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtask_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_counters:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpi_counters[task_index][task] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/adv_rl (Copy)/features/successor.py:227\u001b[0m, in \u001b[0;36mSF.GPI_w\u001b[0;34m(self, state, w)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mGPI_w\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, w):\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    Implements generalized policy improvement according to [1]. \u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    np.ndarray : the tasks that are active in each state of state_batch in GPi\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     psi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_successors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     w_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(w)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [F,1]\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# compute q: [B, n_tasks, n_actions, 1] -> squeeze last dim\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/adv_rl (Copy)/features/deep.py:109\u001b[0m, in \u001b[0;36mDeepSF.get_successors\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    107\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 109\u001b[0m         psi \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# torch [B, A, F]\u001b[39;00m\n\u001b[1;32m    110\u001b[0m         all_psi\u001b[38;5;241m.\u001b[39mappend(psi\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# all_psi: list of length n_tasks; each element [B, A, F]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Downloads/adv_rl (Copy)/features/deep.py:34\u001b[0m, in \u001b[0;36mSFNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 34\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m(x)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Reshape to [batch, n_actions, n_features]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl/lib/python3.10/site-packages/torch/nn/modules/module.py:1949\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1944\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[0;32m-> 1949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1951\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train_randomized(\n",
    "            train_tasks=train_tasks,\n",
    "            n_total_steps=n_samples * len(train_tasks),\n",
    "            viewers=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a5d687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
